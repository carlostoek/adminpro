---
wave: 2
depends_on: 18-01
files_modified:
  - bot/services/test_runner.py
  - bot/utils/test_report.py
  - bot/handlers/admin/tests.py
autonomous: true
---

# Plan 18-02: Test Reporting with Coverage and Detailed Results

## Objective

Enhance the test runner system with comprehensive reporting capabilities including coverage analysis, detailed failure reports, historical tracking, and formatted output suitable for Telegram messages.

## Context

The basic test runner from Plan 18-01 executes tests and returns raw results. This plan adds sophisticated reporting that transforms raw pytest output into actionable intelligence for administrators.

**Current State:**
- TestRunnerService executes tests and returns TestResult dataclass
- Basic formatting exists but lacks coverage integration
- No historical tracking of test runs
- Failure details are truncated in Telegram output

**Why This Matters:**
- Coverage metrics identify untested code paths
- Historical tracking shows trends over time
- Detailed failure reports enable faster debugging
- Formatted Telegram reports improve readability

## Success Criteria

1. Coverage percentage displayed in test reports
2. Failed tests show file:line and error message excerpts
3. Reports include duration trends (vs previous runs)
4. HTML coverage reports generated and storable
5. Telegram messages use formatting (bold, code blocks, emojis)

## Tasks

### Task 1: Enhance TestResult with Coverage Data

**File:** `bot/services/test_runner.py`

Extend the TestResult dataclass and parsing to include coverage:

```python
@dataclass
class TestResult:
    """Resultado completo de una ejecucion de tests."""
    returncode: int
    passed: int
    failed: int
    errors: int
    skipped: int
    total: int
    duration_seconds: float
    stdout: str
    stderr: str
    coverage_percent: Optional[float] = None
    coverage_by_module: Optional[Dict[str, float]] = None
    failed_tests: Optional[List[Dict]] = None  # Detailed failure info
    warnings: Optional[List[str]] = None

    @property
    def success_rate(self) -> float:
        """Porcentaje de tests exitosos."""
        if self.total == 0:
            return 0.0
        return (self.passed / self.total) * 100

    @property
    def coverage_status(self) -> str:
        """Estado del coverage como emoji + texto."""
        if self.coverage_percent is None:
            return "‚ùì No disponible"
        elif self.coverage_percent >= 80:
            return f"‚úÖ {self.coverage_percent:.1f}%"
        elif self.coverage_percent >= 60:
            return f"‚ö†Ô∏è {self.coverage_percent:.1f}%"
        else:
            return f"‚ùå {self.coverage_percent:.1f}%"

    def get_failed_test_details(self, limit: int = 5) -> List[Dict]:
        """
        Extrae detalles de tests fallidos del output.

        Returns:
            Lista de dicts con: name, file, line, error_type, message
        """
        # Parse pytest output to extract failure details
        # Look for patterns like:
        # "FAILED tests/test_x.py::test_name - AssertionError: message"
        # And extract traceback information
        pass

    def get_slowest_tests(self, limit: int = 5) -> List[Dict]:
        """
        Extrae los tests mas lentos del output.

        Requires pytest --durations flag.
        """
        pass
```

### Task 2: Create Test Report Utility Module

**File:** `bot/utils/test_report.py`

Create a dedicated module for report formatting and historical tracking:

```python
"""
Test Report Utilities

Utilidades para formatear reportes de tests, generar HTML,
y mantener historial de ejecuciones.
"""

import json
import logging
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from bot.services.test_runner import TestResult

logger = logging.getLogger(__name__)


@dataclass
class TestRunRecord:
    """Registro historico de una ejecucion de tests."""
    timestamp: datetime
    triggered_by: str  # 'cli', 'telegram:<user_id>', 'scheduled'
    result: TestResult
    git_commit: Optional[str] = None
    git_branch: Optional[str] = None


class TestReportHistory:
    """
    Almacena y recupera historial de ejecuciones de tests.

    Usa archivo JSON para persistencia simple.
    """

    def __init__(self, storage_path: Path = None):
        self.storage_path = storage_path or Path("data/test_history.json")
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        self._cache: List[TestRunRecord] = []

    async def add_record(self, record: TestRunRecord) -> None:
        """Agrega un registro al historial."""
        self._cache.append(record)
        await self._save()

    async def get_recent(self, limit: int = 10) -> List[TestRunRecord]:
        """Retorna los N registros mas recientes."""
        if not self._cache:
            await self._load()
        return sorted(
            self._cache,
            key=lambda r: r.timestamp,
            reverse=True
        )[:limit]

    async def get_trend(self, days: int = 7) -> dict:
        """
        Calcula tendencias de tests en los ultimos N dias.

        Returns:
            Dict con: avg_duration, avg_pass_rate, trend_direction
        """
        pass

    async def _load(self) -> None:
        """Carga historial desde disco."""
        if self.storage_path.exists():
            try:
                data = json.loads(self.storage_path.read_text())
                self._cache = [
                    TestRunRecord(
                        timestamp=datetime.fromisoformat(r["timestamp"]),
                        triggered_by=r["triggered_by"],
                        result=TestResult(**r["result"]),
                        git_commit=r.get("git_commit"),
                        git_branch=r.get("git_branch")
                    )
                    for r in data
                ]
            except Exception as e:
                logger.error(f"Error cargando historial: {e}")
                self._cache = []

    async def _save(self) -> None:
        """Guarda historial a disco."""
        # Keep only last 100 records
        records = self._cache[-100:]
        data = [
            {
                "timestamp": r.timestamp.isoformat(),
                "triggered_by": r.triggered_by,
                "result": asdict(r.result),
                "git_commit": r.git_commit,
                "git_branch": r.git_branch
            }
            for r in records
        ]
        self.storage_path.write_text(json.dumps(data, indent=2))


class TestReportFormatter:
    """
    Formatea resultados de tests para diferentes salidas.
    """

    @staticmethod
    def format_telegram(result: TestResult, history: Optional[TestRunRecord] = None) -> str:
        """
        Formatea resultado para mensaje de Telegram (HTML).

        Args:
            result: Resultado de tests
            history: Registro historico previo para comparacion

        Returns:
            Texto formateado en HTML
        """
        lines = []

        # Header with status
        if result.success:
            lines.append("‚úÖ <b>Todos los tests pasaron</b>")
        else:
            lines.append("‚ùå <b>Algunos tests fallaron</b>")

        lines.append("")

        # Summary stats
        lines.append(f"üìä <b>Resumen:</b>")
        lines.append(f"   ‚Ä¢ Total: {result.total}")
        lines.append(f"   ‚Ä¢ ‚úÖ Pasaron: {result.passed}")
        if result.failed > 0:
            lines.append(f"   ‚Ä¢ ‚ùå Fallaron: {result.failed}")
        if result.errors > 0:
            lines.append(f"   ‚Ä¢ üí• Errores: {result.errors}")
        if result.skipped > 0:
            lines.append(f"   ‚Ä¢ ‚è≠Ô∏è Omitidos: {result.skipped}")

        lines.append("")

        # Coverage
        if result.coverage_percent is not None:
            lines.append(f"üìà <b>Coverage:</b> {result.coverage_status}")
            lines.append("")

        # Duration with trend
        duration_str = f"‚è±Ô∏è <b>Duracion:</b> {result.duration_seconds:.1f}s"
        if history:
            prev_duration = history.result.duration_seconds
            diff = result.duration_seconds - prev_duration
            if abs(diff) > 1.0:  # Only show if difference > 1s
                trend = "üìà" if diff > 0 else "üìâ"
                duration_str += f" ({trend} {abs(diff):.1f}s vs anterior)"
        lines.append(duration_str)

        # Failed tests (first 3)
        if result.failed_tests:
            lines.append("")
            lines.append("‚ùå <b>Tests fallidos:</b>")
            for test in result.failed_tests[:3]:
                lines.append(f"   ‚Ä¢ <code>{test['name']}</code>")
                if 'error_type' in test:
                    lines.append(f"     <i>{test['error_type']}</i>")

        if len(result.failed_tests) > 3:
            lines.append(f"   <i>... y {len(result.failed_tests) - 3} mas</i>")

        return "\n".join(lines)

    @staticmethod
    def format_console(result: TestResult) -> str:
        """Formatea resultado para consola (con colores ANSI)."""
        pass

    @staticmethod
    def format_json(result: TestResult) -> str:
        """Formatea resultado como JSON."""
        return json.dumps(asdict(result), indent=2, default=str)

    @staticmethod
    def generate_html_report(result: TestResult, output_path: Path) -> None:
        """
        Genera reporte HTML completo con coverage.

        Args:
            result: Resultado de tests
            output_path: Path para guardar el HTML
        """
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Test Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ margin-bottom: 30px; }}
                .success {{ color: #28a745; }}
                .failure {{ color: #dc3545; }}
                .stats {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; }}
                .stat-box {{ padding: 20px; border-radius: 8px; background: #f8f9fa; }}
                .stat-value {{ font-size: 2em; font-weight: bold; }}
                .failures {{ margin-top: 30px; }}
                pre {{ background: #f8f9fa; padding: 15px; overflow-x: auto; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1 class="{'success' if result.success else 'failure'}">
                    {'‚úì Tests Passed' if result.success else '‚úó Tests Failed'}
                </h1>
                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>

            <div class="stats">
                <div class="stat-box">
                    <div class="stat-value">{result.total}</div>
                    <div>Total Tests</div>
                </div>
                <div class="stat-box">
                    <div class="stat-value" style="color: #28a745">{result.passed}</div>
                    <div>Passed</div>
                </div>
                <div class="stat-box">
                    <div class="stat-value" style="color: #dc3545">{result.failed}</div>
                    <div>Failed</div>
                </div>
                <div class="stat-box">
                    <div class="stat-value">{result.coverage_percent or 'N/A'}%</div>
                    <div>Coverage</div>
                </div>
            </div>

            {'<div class="failures"><h2>Failures</h2>' + ''.join(f"<h3>{t['name']}</h3><pre>{t.get('traceback', 'No traceback')}</pre>" for t in result.failed_tests) + '</div>' if result.failed_tests else ''}
        </body>
        </html>
        """
        output_path.write_text(html)
```

### Task 3: Update TestRunnerService with Enhanced Reporting

**File:** `bot/services/test_runner.py`

Integrate the new reporting capabilities:

```python
class TestRunnerService:
    """
    Servicio mejorado para ejecutar tests con reportes detallados.
    """

    def __init__(
        self,
        session: AsyncSession,
        project_root: Optional[Path] = None,
        history: Optional[TestReportHistory] = None
    ):
        self.session = session
        self.project_root = project_root or Path(__file__).parent.parent.parent
        self.history = history or TestReportHistory()
        self.formatter = TestReportFormatter()
        self._lock = asyncio.Lock()

    async def run_tests_with_report(
        self,
        triggered_by: str,
        test_paths: Optional[List[str]] = None,
        coverage: bool = False,
        generate_html: bool = False,
        html_output: Optional[Path] = None
    ) -> Tuple[TestResult, str]:
        """
        Ejecuta tests y genera reporte completo.

        Args:
            triggered_by: Identificador de quien ejecuto ('cli', 'telegram:123')
            test_paths: Paths especificos a testear
            coverage: Si True, incluye coverage
            generate_html: Si True, genera reporte HTML
            html_output: Path para el reporte HTML

        Returns:
            Tuple de (TestResult, telegram_formatted_report)
        """
        # Get git info
        git_commit = await self._get_git_commit()
        git_branch = await self._get_git_branch()

        # Run tests
        result = await self.run_tests(
            test_paths=test_paths,
            coverage=coverage,
            extra_args=["--durations=5"]  # Show 5 slowest tests
        )

        # Generate HTML report if requested
        if generate_html:
            output = html_output or Path("data/reports/test_report.html")
            output.parent.mkdir(parents=True, exist_ok=True)
            self.formatter.generate_html_report(result, output)
            logger.info(f"Reporte HTML generado: {output}")

        # Get previous record for comparison
        recent = await self.history.get_recent(1)
        previous = recent[0] if recent else None

        # Format Telegram report
        telegram_report = self.formatter.format_telegram(result, previous)

        # Save to history
        record = TestRunRecord(
            timestamp=datetime.now(),
            triggered_by=triggered_by,
            result=result,
            git_commit=git_commit,
            git_branch=git_branch
        )
        await self.history.add_record(record)

        return result, telegram_report

    async def _get_git_commit(self) -> Optional[str]:
        """Obtiene el hash del commit actual."""
        try:
            proc = await asyncio.create_subprocess_exec(
                "git", "rev-parse", "--short", "HEAD",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.project_root
            )
            stdout, _ = await proc.communicate()
            return stdout.decode().strip() if proc.returncode == 0 else None
        except Exception:
            return None

    async def _get_git_branch(self) -> Optional[str]:
        """Obtiene la rama actual de git."""
        try:
            proc = await asyncio.create_subprocess_exec(
                "git", "branch", "--show-current",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.project_root
            )
            stdout, _ = await proc.communicate()
            return stdout.decode().strip() if proc.returncode == 0 else None
        except Exception:
            return None
```

### Task 4: Update Telegram Handler

**File:** `bot/handlers/admin/tests.py`

Update the handler to use enhanced reporting:

```python
@tests_router.message(Command("run_tests"))
async def cmd_run_tests(message: Message, session: AsyncSession):
    """
    Ejecuta tests con reporte detallado y opciones avanzadas.

    Uso:
        /run_tests - Ejecuta todos los tests
        /run_tests smoke - Solo smoke tests
        /run_tests coverage - Con reporte de coverage
        /run_tests html - Genera reporte HTML
        /run_tests trend - Muestra tendencia historica
    """
    args = message.text.split()[1:] if message.text else []
    coverage = "coverage" in args
    generate_html = "html" in args
    marker = None
    test_paths = None

    if "smoke" in args:
        marker = "smoke"
    elif "system" in args:
        test_paths = ["tests/test_system/"]

    # Show trend if requested
    if "trend" in args:
        history = TestReportHistory()
        trend = await history.get_trend(days=7)
        report = (
            "üìà <b>Tendencia de Tests (7 dias)</b>\n\n"
            f"‚Ä¢ Promedio exito: {trend.get('avg_pass_rate', 0):.1f}%\n"
            f"‚Ä¢ Promedio duracion: {trend.get('avg_duration', 0):.1f}s\n"
            f"‚Ä¢ Tendencia: {trend.get('trend_direction', 'estable')}"
        )
        await message.answer(report, parse_mode="HTML")
        return

    # Send initial status
    status_msg = await message.answer(
        "üß™ <b>Iniciando ejecucion de tests...</b>\n"
        f"<i>Coverage: {'Si' if coverage else 'No'} | "
        f"HTML: {'Si' if generate_html else 'No'}</i>"
    )

    try:
        # Run tests with full reporting
        runner = TestRunnerService(session)
        result, report = await runner.run_tests_with_report(
            triggered_by=f"telegram:{message.from_user.id}",
            test_paths=test_paths,
            coverage=coverage,
            generate_html=generate_html
        )

        # Delete status message
        await status_msg.delete()

        # Send main report
        if len(report) > 4000:
            # Split long reports
            parts = []
            current = ""
            for line in report.split("\n"):
                if len(current) + len(line) + 1 > 4000:
                    parts.append(current)
                    current = line
                else:
                    current += "\n" + line if current else line
            if current:
                parts.append(current)

            for i, part in enumerate(parts):
                header = f"üìä <b>Reporte (parte {i+1}/{len(parts)})</b>\n\n"
                await message.answer(header + part, parse_mode="HTML")
        else:
            await message.answer(report, parse_mode="HTML")

        # Send HTML report file if generated
        if generate_html and Path("data/reports/test_report.html").exists():
            await message.answer_document(
                document=FSInputFile("data/reports/test_report.html"),
                caption="üìÑ Reporte HTML completo"
            )

        # Show failure details button if needed
        if result.failed > 0 or result.errors > 0:
            keyboard = InlineKeyboardMarkup(inline_keyboard=[
                [InlineKeyboardButton(
                    text="üìã Ver traceback completo",
                    callback_data=f"tests:failures:{datetime.now().timestamp()}"
                )]
            ])
            await message.answer(
                "‚ùå Algunos tests fallaron. "
                "Presiona el boton para ver detalles.",
                reply_markup=keyboard
            )

    except Exception as e:
        logger.exception("Error en ejecucion de tests")
        await status_msg.edit_text(
            f"‚ùå <b>Error ejecutando tests</b>\n\n"
            f"<code>{str(e)[:500]}</code>"
        )


@tests_router.callback_query(F.data.startswith("tests:failures:"))
async def callback_show_failures(callback: CallbackQuery, session: AsyncSession):
    """Muestra detalles completos de fallos como documento."""
    # Implementation: retrieve last test result, format failures as text file
    # Send as document to avoid message length limits
    pass
```

## Verification

**Manual Verification:**
1. Run tests with coverage: `/run_tests coverage`
2. Verify coverage percentage appears in report
3. Introduce a test failure and verify it's shown with file:line
4. Run `/run_tests trend` and verify historical data displays
5. Run `/run_tests html` and verify HTML file is sent

**Automated Verification:**
```python
async def test_coverage_parsing():
    """Test that coverage output is correctly parsed."""
    result = TestResult(
        returncode=0,
        passed=10, failed=0, errors=0, skipped=0, total=10,
        duration_seconds=5.0,
        stdout="TOTAL 85%",
        stderr=""
    )
    # Parse coverage from stdout
    assert result.coverage_percent == 85.0

async def test_telegram_formatting():
    """Test Telegram report formatting."""
    result = TestResult(
        returncode=1,
        passed=8, failed=2, errors=0, skipped=0, total=10,
        duration_seconds=10.0,
        stdout="",
        stderr="",
        coverage_percent=75.0,
        failed_tests=[
            {"name": "test_x", "error_type": "AssertionError"}
        ]
    )
    report = TestReportFormatter.format_telegram(result)
    assert "‚ùå" in report
    assert "75.0%" in report
    assert "test_x" in report
```

## Anti-Patterns to Avoid

1. **Don't store full stdout in history** - Too large, store only summary
2. **Don't generate HTML on every run** - Make it optional (html flag)
3. **Don't show raw tracebacks in Telegram** - Format and truncate
4. **Don't block on history save** - Use async file operations
5. **Don't expose sensitive paths** - Sanitize file paths in reports

## Notes

- HTML reports use inline CSS for email compatibility
- History file is JSON for easy manual inspection
- Coverage parsing depends on pytest-cov output format
- Git info is optional - works fine outside git repos

## References

- pytest-cov documentation: https://pytest-cov.readthedocs.io/
- aiogram HTML formatting: https://core.telegram.org/bots/api#html-style
