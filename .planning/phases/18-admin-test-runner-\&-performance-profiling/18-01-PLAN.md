---
wave: 1
depends_on: 17-system-tests
files_modified:
  - scripts/run_tests.py
  - bot/handlers/admin/tests.py
  - bot/handlers/admin/__init__.py
autonomous: true
---

# Plan 18-01: Admin Test Runner Script and Telegram Command

## Objective

Create a comprehensive test runner system that allows executing pytest suites both from the command line via a script and from Telegram via the `/run_tests` admin command. The system must provide real-time feedback and handle test execution safely in an isolated subprocess.

## Context

The project has a comprehensive test suite in the `tests/` directory with 50+ tests covering system startup, configuration, handlers, services, and user flows. Currently, tests can only be run manually with `pytest`. Administrators need the ability to:

1. Run tests locally during development via CLI script
2. Run tests remotely from Telegram to verify production health
3. Get immediate feedback on test results without SSH access

**Current State:**
- Tests located in `tests/` with subdirectories for system, integration, and infrastructure tests
- pytest configured in `pytest.ini` with async_mode=auto
- Coverage reporting available via pytest-cov
- Admin handlers in `bot/handlers/admin/` with existing auth middleware

**Why This Matters:**
- Enables rapid verification of bot health without server access
- Allows non-technical stakeholders to verify system status
- Provides confidence before deployments
- Facilitates remote troubleshooting

## Success Criteria

1. `python scripts/run_tests.py` executes all tests and displays results
2. `/run_tests` command in Telegram executes tests (admin-only)
3. Test execution runs in isolated subprocess (does not block bot)
4. Results include: pass count, fail count, error count, duration
5. Failed tests show traceback excerpts in Telegram message

## Tasks

### Task 1: Create CLI Test Runner Script

**File:** `scripts/run_tests.py`

Create a command-line test runner with argument parsing, subprocess execution, and formatted output:

```python
#!/usr/bin/env python3
"""
CLI Test Runner Script

Ejecuta tests de pytest desde la linea de comandos con opciones
de coverage, filtros por directorio, y reportes formateados.

Uso:
    python scripts/run_tests.py
    python scripts/run_tests.py --coverage
    python scripts/run_tests.py tests/test_system/
    python scripts/run_tests.py --marker "slow"
"""

import argparse
import asyncio
import subprocess
import sys
from pathlib import Path
from typing import List, Optional, Tuple


class TestRunner:
    """Ejecutor de tests con soporte para pytest y coverage."""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.pytest_path = self._find_pytest()

    def _find_pytest(self) -> str:
        """Encuentra el ejecutable de pytest."""
        # Implementation: check venv, then PATH
        pass

    async def run_tests(
        self,
        test_paths: Optional[List[str]] = None,
        coverage: bool = False,
        marker: Optional[str] = None,
        verbose: bool = False,
        junit_xml: Optional[str] = None
    ) -> Tuple[int, str, str]:
        """
        Ejecuta tests en subprocess y retorna resultado.

        Args:
            test_paths: Lista de paths a ejecutar (None = todos)
            coverage: Si True, genera reporte de coverage
            marker: Ejecutar solo tests con este marker
            verbose: Salida verbose de pytest
            junit_xml: Path para output XML (opcional)

        Returns:
            Tuple de (returncode, stdout, stderr)
        """
        # Build pytest command
        # Run in subprocess with asyncio.create_subprocess_exec
        # Capture stdout/stderr
        # Return results
        pass

    def parse_results(self, stdout: str, stderr: str) -> dict:
        """
        Parsea la salida de pytest para extraer metricas.

        Returns:
            Dict con: passed, failed, errors, skipped, total, duration
        """
        pass

    def format_report(self, results: dict) -> str:
        """Formatea resultados para display en consola."""
        pass


async def main():
    """Punto de entrada del script CLI."""
    parser = argparse.ArgumentParser(
        description="Ejecutor de tests para el bot de Telegram"
    )
    parser.add_argument(
        "paths",
        nargs="*",
        help="Paths a ejecutar (default: todos los tests)"
    )
    parser.add_argument(
        "--coverage",
        action="store_true",
        help="Generar reporte de coverage"
    )
    parser.add_argument(
        "--marker",
        help="Ejecutar solo tests con este marker"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Salida verbose"
    )
    parser.add_argument(
        "--junit-xml",
        help="Generar reporte JUnit XML"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output en formato JSON"
    )

    args = parser.parse_args()

    # Execute tests
    # Print formatted results
    # Exit with pytest return code
```

**Key Features:**
- Subprocess execution prevents test crashes from affecting the runner
- Async/await support for non-blocking execution
- JSON output option for programmatic consumption
- JUnit XML output for CI/CD integration
- Automatic pytest discovery in virtualenv

### Task 2: Create Test Runner Service

**File:** `bot/services/test_runner.py`

Create a service class that encapsulates test execution logic for use by both CLI and Telegram handlers:

```python
"""
Test Runner Service

Servicio para ejecutar tests desde el bot, con soporte para
reportes formateados y notificaciones a administradores.
"""

import asyncio
import logging
import subprocess
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


@dataclass
class TestResult:
    """Resultado de una ejecucion de tests."""
    returncode: int
    passed: int
    failed: int
    errors: int
    skipped: int
    total: int
    duration_seconds: float
    stdout: str
    stderr: str
    coverage_percent: Optional[float] = None

    @property
    def success(self) -> bool:
        """True si todos los tests pasaron."""
        return self.returncode == 0 and self.failed == 0 and self.errors == 0

    @property
    def summary(self) -> str:
        """Resumen en una linea."""
        return (
            f"Tests: {self.total} total, "
            f"{self.passed} passed, "
            f"{self.failed} failed, "
            f"{self.errors} errors, "
            f"{self.skipped} skipped"
        )


class TestRunnerService:
    """
    Servicio para ejecutar tests pytest desde el bot.

    Ejecuta tests en subprocess aislado para evitar que
    errores afecten el proceso principal del bot.
    """

    def __init__(self, session: AsyncSession, project_root: Optional[Path] = None):
        self.session = session
        self.project_root = project_root or Path(__file__).parent.parent.parent
        self._lock = asyncio.Lock()

    async def run_tests(
        self,
        test_paths: Optional[List[str]] = None,
        coverage: bool = False,
        marker: Optional[str] = None,
        timeout: int = 300
    ) -> TestResult:
        """
        Ejecuta tests y retorna resultado estructurado.

        Args:
            test_paths: Paths especificos a testear (None = todos)
            coverage: Si True, incluye coverage
            marker: Marker de pytest para filtrar tests
            timeout: Timeout en segundos para la ejecucion

        Returns:
            TestResult con metricas y output
        """
        async with self._lock:
            # Build command
            cmd = ["python", "-m", "pytest"]

            if coverage:
                cmd.extend(["--cov=bot", "--cov-report=term"])

            if marker:
                cmd.extend(["-m", marker])

            cmd.extend(test_paths or ["tests/"])
            cmd.extend(["-v", "--tb=short"])

            # Execute in subprocess
            logger.info(f"Ejecutando tests: {' '.join(cmd)}")

            try:
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=self.project_root
                )

                stdout, stderr = await asyncio.wait_for(
                    proc.communicate(),
                    timeout=timeout
                )

                stdout_str = stdout.decode("utf-8", errors="replace")
                stderr_str = stderr.decode("utf-8", errors="replace")

                # Parse results
                result = self._parse_results(
                    proc.returncode or 0,
                    stdout_str,
                    stderr_str
                )

                logger.info(f"Tests completados: {result.summary}")
                return result

            except asyncio.TimeoutError:
                logger.error(f"Tests timeout despues de {timeout}s")
                proc.kill()
                return TestResult(
                    returncode=-1,
                    passed=0, failed=0, errors=1, skipped=0, total=0,
                    duration_seconds=timeout,
                    stdout="",
                    stderr=f"Timeout despues de {timeout} segundos"
                )

    def _parse_results(
        self,
        returncode: int,
        stdout: str,
        stderr: str
    ) -> TestResult:
        """Parsea output de pytest para extraer metricas."""
        # Parse lines like: "50 passed, 2 failed, 1 error in 12.34s"
        # Extract coverage percentage if present
        pass

    async def run_smoke_tests(self) -> TestResult:
        """Ejecuta solo smoke tests (verificacion rapida)."""
        return await self.run_tests(marker="smoke", timeout=60)

    async def run_system_tests(self) -> TestResult:
        """Ejecuta tests del sistema."""
        return await self.run_tests(test_paths=["tests/test_system/"])

    def format_telegram_report(self, result: TestResult, max_length: int = 4000) -> str:
        """
        Formatea resultado para enviar por Telegram.

        Args:
            result: Resultado de tests
            max_length: Longitud maxima del mensaje

        Returns:
            Texto formateado en HTML
        """
        # Format with emojis and HTML
        # Truncate if exceeds max_length
        # Include failed test names
        pass
```

### Task 3: Create Telegram Handler for /run_tests

**File:** `bot/handlers/admin/tests.py`

Create the admin handler that exposes test execution via Telegram:

```python
"""
Admin Test Runner Handler

Handler para ejecutar tests desde Telegram via comando /run_tests.
Solo accesible para administradores.
"""

import logging
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import Message, CallbackQuery
from sqlalchemy.ext.asyncio import AsyncSession

from bot.services.container import ServiceContainer
from bot.services.test_runner import TestRunnerService
from bot.middlewares import AdminAuthMiddleware

logger = logging.getLogger(__name__)

# Router para handlers de tests
tests_router = Router(name="admin_tests")

# Aplicar middleware de admin
tests_router.message.middleware(AdminAuthMiddleware())
tests_router.callback_query.middleware(AdminAuthMiddleware())


@tests_router.message(Command("run_tests"))
async def cmd_run_tests(message: Message, session: AsyncSession):
    """
    Ejecuta tests y envia reporte al admin.

    Uso:
        /run_tests - Ejecuta todos los tests
        /run_tests smoke - Ejecuta solo smoke tests
        /run_tests system - Ejecuta tests de sistema
        /run_tests coverage - Ejecuta con reporte de coverage

    Args:
        message: Mensaje del comando
        session: Sesion de BD inyectada
    """
    logger.info(f"ğŸ§ª Admin {message.from_user.id} solicito ejecucion de tests")

    # Parse arguments
    args = message.text.split()[1:] if message.text else []
    coverage = "coverage" in args
    marker = None
    test_paths = None

    if "smoke" in args:
        marker = "smoke"
    elif "system" in args:
        test_paths = ["tests/test_system/"]

    # Send "running" message
    status_msg = await message.answer(
        "ğŸ§ª <b>Ejecutando tests...</b>\n\n"
        "Esto puede tomar unos minutos."
    )

    try:
        # Create service and run tests
        runner = TestRunnerService(session)
        result = await runner.run_tests(
            test_paths=test_paths,
            coverage=coverage,
            marker=marker
        )

        # Format report
        report = runner.format_telegram_report(result)

        # Delete status message and send results
        await status_msg.delete()

        # Split if too long
        if len(report) > 4000:
            parts = [report[i:i+4000] for i in range(0, len(report), 4000)]
            for i, part in enumerate(parts):
                header = f"ğŸ“Š <b>Reporte de Tests (parte {i+1}/{len(parts)})</b>\n\n"
                await message.answer(header + part, parse_mode="HTML")
        else:
            await message.answer(report, parse_mode="HTML")

        # If there are failures, offer to see details
        if result.failed > 0 or result.errors > 0:
            from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton
            keyboard = InlineKeyboardMarkup(inline_keyboard=[
                [InlineKeyboardButton(
                    text="ğŸ“‹ Ver detalles de fallos",
                    callback_data="tests:show_failures"
                )]
            ])
            await message.answer(
                "âŒ Algunos tests fallaron. Â¿Deseas ver los detalles?",
                reply_markup=keyboard
            )

    except Exception as e:
        logger.exception("Error ejecutando tests")
        await status_msg.edit_text(
            f"âŒ <b>Error ejecutando tests</b>\n\n"
            f"<code>{str(e)}</code>"
        )


@tests_router.callback_query(F.data == "tests:show_failures")
async def callback_show_failures(callback: CallbackQuery, session: AsyncSession):
    """Muestra detalles de tests fallidos."""
    # Implementation: retrieve last test result and show failures
    pass


@tests_router.message(Command("test_status"))
async def cmd_test_status(message: Message, session: AsyncSession):
    """
    Muestra estado del sistema de tests.

    Indica si el entorno de tests esta configurado correctamente
    y cuantos tests hay disponibles.
    """
    # Implementation: count tests, check pytest availability
    pass
```

### Task 4: Register Handler in Admin Router

**File:** `bot/handlers/admin/__init__.py`

Add the tests router to the admin handlers:

```python
# bot/handlers/admin/__init__.py

from bot.handlers.admin.main import admin_router
from bot.handlers.admin.tests import tests_router

# Include tests router
admin_router.include_router(tests_router)

__all__ = ["admin_router"]
```

## Verification

**Manual Verification:**
1. Run `python scripts/run_tests.py --help` and verify options display
2. Run `python scripts/run_tests.py tests/test_system/test_startup.py -v` and verify output
3. Run `python scripts/run_tests.py --coverage` and verify coverage report
4. In Telegram, send `/run_tests smoke` as admin and verify execution
5. Verify non-admin users cannot execute `/run_tests`

**Automated Verification:**
```python
# Test CLI runner
async def test_cli_runner():
    runner = TestRunner(Path("/project"))
    result = await runner.run_tests(["tests/test_system/test_startup.py"])
    assert result.returncode == 0
    assert result.passed > 0

# Test Telegram handler
async def test_run_tests_command(admin_user, test_session):
    # Mock message
    message = MagicMock()
    message.from_user.id = admin_user.id
    message.text = "/run_tests smoke"
    message.answer = AsyncMock()

    await cmd_run_tests(message, test_session)
    message.answer.assert_called()
    assert "Ejecutando tests" in message.answer.call_args[0][0]
```

## Anti-Patterns to Avoid

1. **Don't run tests in the main process** - Always use subprocess to prevent crashes
2. **Don't expose test output to non-admins** - Verify AdminAuthMiddleware is applied
3. **Don't hardcode paths** - Use Path and project root detection
4. **Don't ignore timeouts** - Always set and handle execution timeouts
5. **Don't block the event loop** - Use asyncio subprocess, not sync subprocess

## Notes

- Test execution may take 30-120 seconds depending on coverage
- Large output will be split into multiple Telegram messages
- Coverage requires the `pytest-cov` package (already in requirements)
- The subprocess runs with the same Python interpreter as the bot

## References

- Phase 17 system tests: `tests/test_system/`
- pytest documentation: https://docs.pytest.org/
- aiogram handlers: `bot/handlers/admin/main.py`
