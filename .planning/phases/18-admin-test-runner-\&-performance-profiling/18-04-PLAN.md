---
wave: 3
depends_on: 18-02
files_modified:
  - scripts/migrate_to_postgres.py
  - bot/utils/query_analyzer.py
  - bot/handlers/admin/profile.py
  - bot/database/engine.py
  - bot/services/subscription.py
  - bot/services/channel.py
autonomous: true
---

# Plan 18-04: SQLite to PostgreSQL Migration and N+1 Query Detection

## Objective

Create a robust data migration script to transfer all data from SQLite to PostgreSQL without loss, and implement N+1 query detection with SQLAlchemy logging to identify and fix performance bottlenecks in database access patterns.

## Context

The bot currently uses SQLite for simplicity, but PostgreSQL will be needed for production scaling. Additionally, as the codebase grows, N+1 query problems (where accessing related data causes multiple queries instead of one) can severely impact performance.

**Current State:**
- SQLite database with WAL mode enabled
- SQLAlchemy 2.0 with async support
- Services use lazy loading for relationships
- No query logging or analysis tools
- PostgreSQL driver (asyncpg) already in requirements

**Why This Matters:**
- PostgreSQL provides better concurrency for high-traffic bots
- N+1 queries can make a handler take O(n) queries instead of O(1)
- Early detection of query problems prevents production issues
- Migration must be lossless and reversible

## Success Criteria

1. `python scripts/migrate_to_postgres.py --source bot.db --target postgresql://...` migrates all data
2. Migration validates row counts match between source and target
3. N+1 query detection logs warnings when detected
4. Services use eager loading (selectinload) where appropriate
5. Query analyzer provides actionable optimization suggestions

## Tasks

### Task 1: Create Query Analyzer Utility

**File:** `bot/utils/query_analyzer.py`

Create a utility to detect and warn about N+1 queries:

```python
"""
Query Analyzer Utilities

Detecta N+1 queries y problemas de rendimiento en accesos
a base de datos mediante SQLAlchemy event listeners.
"""

import logging
import time
from collections import defaultdict
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set

from sqlalchemy import event
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

logger = logging.getLogger(__name__)


@dataclass
class QueryInfo:
    """Informacion sobre una query ejecutada."""
    statement: str
    duration_ms: float
    stack_trace: List[str]
    timestamp: float


@dataclass
class NPlusOnePattern:
    """Patron N+1 detectado."""
    table: str
    operation: str
    count: int
    sample_query: str
    location: str
    suggestion: str


class QueryAnalyzer:
    """
    Analizador de queries para detectar problemas de rendimiento.

    Usa SQLAlchemy events para monitorear queries y detectar
    patrones problematicos como N+1.

    Uso:
        analyzer = QueryAnalyzer()
        analyzer.attach_to_session(session)

        # Run your code

        issues = analyzer.get_issues()
        analyzer.detach()  # Clean up
    """

    def __init__(self, n_plus_one_threshold: int = 5):
        self.n_plus_one_threshold = n_plus_one_threshold
        self.queries: List[QueryInfo] = []
        self.query_counts: Dict[str, int] = defaultdict(int)
        self._handlers: List = []
        self._session: Optional[AsyncSession] = None

    def attach_to_session(self, session: AsyncSession) -> None:
        """Attach event listeners a una sesion."""
        self._session = session

        @event.listens_for(session.sync_session, "do_orm_execute")
        def on_orm_execute(execute_state):
            self._count_orm_execute(execute_state)

        @event.listens_for(session.bind.sync_engine, "before_cursor_execute")
        def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            context._query_start_time = time.time()

        @event.listens_for(session.bind.sync_engine, "after_cursor_execute")
        def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            duration = (time.time() - context._query_start_time) * 1000
            self._record_query(statement, duration)

        self._handlers.extend([on_orm_execute, before_cursor_execute, after_cursor_execute])

    def detach(self) -> None:
        """Remove event listeners."""
        # SQLAlchemy doesn't provide easy removal, handlers will be GC'd with session
        self._session = None

    def _count_orm_execute(self, execute_state) -> None:
        """Cuenta ejecuciones ORM para detectar N+1."""
        # Track lazy loads
        if execute_state.is_select:
            mapper = getattr(execute_state, 'mapper', None)
            if mapper:
                table_name = mapper.persist_selectable.name
                self.query_counts[f"select_{table_name}"] += 1

    def _record_query(self, statement: str, duration_ms: float) -> None:
        """Registra una query ejecutada."""
        import traceback

        # Get stack trace (skip SQLAlchemy internals)
        stack = traceback.extract_stack()
        relevant_frames = [
            f"{frame.filename}:{frame.lineno} in {frame.name}"
            for frame in stack
            if "sqlalchemy" not in frame.filename and "asyncpg" not in frame.filename
        ][-10:]  # Last 10 relevant frames

        query = QueryInfo(
            statement=statement[:500],  # Truncate long statements
            duration_ms=duration_ms,
            stack_trace=relevant_frames,
            timestamp=time.time()
        )
        self.queries.append(query)

        # Log slow queries
        if duration_ms > 100:  # Queries slower than 100ms
            logger.warning(f"Slow query ({duration_ms:.1f}ms): {statement[:200]}...")

    def get_issues(self) -> List[NPlusOnePattern]:
        """
        Analiza queries registradas y retorna problemas detectados.

        Returns:
            Lista de patrones N+1 detectados
        """
        issues = []

        # Detect repeated queries to same table
        table_counts: Dict[str, int] = defaultdict(int)
        for query in self.queries:
            # Extract table name from query
            if "FROM" in query.statement.upper():
                parts = query.statement.upper().split("FROM")
                if len(parts) > 1:
                    table = parts[1].strip().split()[0]
                    table_counts[table] += 1

        # Flag tables with many repeated queries
        for table, count in table_counts.items():
            if count > self.n_plus_one_threshold:
                sample = next(
                    (q.statement for q in self.queries if table in q.statement.upper()),
                    "N/A"
                )
                issues.append(NPlusOnePattern(
                    table=table,
                    operation="SELECT",
                    count=count,
                    sample_query=sample,
                    location=self._find_source_location(table),
                    suggestion=f"Consider using selectinload() for {table} relationships"
                ))

        return issues

    def _find_source_location(self, table: str) -> str:
        """Intenta encontrar el codigo fuente del patron N+1."""
        for query in self.queries:
            if table in query.statement.upper():
                # Find first frame from bot/ directory
                for frame in query.stack_trace:
                    if "bot/" in frame:
                        return frame
                return query.stack_trace[-1] if query.stack_trace else "Unknown"
        return "Unknown"

    def get_summary(self) -> Dict:
        """Retorna resumen de analisis."""
        total_queries = len(self.queries)
        total_duration = sum(q.duration_ms for q in self.queries)
        slow_queries = len([q for q in self.queries if q.duration_ms > 100])

        return {
            "total_queries": total_queries,
            "total_duration_ms": total_duration,
            "avg_duration_ms": total_duration / total_queries if total_queries else 0,
            "slow_queries": slow_queries,
            "n_plus_one_issues": len(self.get_issues()),
            "tables_accessed": len(set(
                self._extract_table(q.statement) for q in self.queries
            ))
        }

    def _extract_table(self, statement: str) -> str:
        """Extrae nombre de tabla de una query SQL."""
        upper = statement.upper()
        if "FROM" in upper:
            return upper.split("FROM")[1].strip().split()[0]
        return "unknown"


@contextmanager
def analyze_queries(n_plus_one_threshold: int = 5):
    """
    Context manager para analizar queries en un bloque de codigo.

    Uso:
        with analyze_queries() as analyzer:
            result = await some_service.get_data()

        issues = analyzer.get_issues()
        if issues:
            print(f"N+1 detected: {issues[0].suggestion}")
    """
    analyzer = QueryAnalyzer(n_plus_one_threshold)
    try:
        yield analyzer
    finally:
        analyzer.detach()


class QueryOptimizationSuggestions:
    """
    Provee sugerencias especificas de optimizacion basadas
    en patrones de queries detectados.
    """

    @staticmethod
    def suggest_eager_loading(relationship_path: str) -> str:
        """Sugiere uso de selectinload para una relacion."""
        return (
            f"Use selectinload({relationship_path}) to avoid N+1. "
            f"Example: select(Model).options(selectinload(Model.relationship))"
        )

    @staticmethod
    def suggest_batch_loading(table: str, current_batch_size: int) -> str:
        """Sugiere batch loading para queries repetidas."""
        return (
            f"Consider batching {table} queries. "
            f"Current pattern suggests {current_batch_size} individual queries."
        )

    @staticmethod
    def suggest_indexing(column: str, table: str) -> str:
        """Sugiere crear un indice para columnas frecuentemente filtradas."""
        return (
            f"Consider adding an index on {table}.{column} "
            f"for faster filtering."
        )


def detect_n_plus_one_in_service(service_method):
    """
    Decorator para detectar N+1 en metodos de servicio.

    Uso:
        @detect_n_plus_one_in_service
        async def get_vip_subscribers(self):
            ...
    """
    async def wrapper(self, *args, **kwargs):
        # Only analyze in debug mode
        import os
        if not os.getenv("DEBUG_QUERIES"):
            return await service_method(self, *args, **kwargs)

        with analyze_queries() as analyzer:
            result = await service_method(self, *args, **kwargs)

        issues = analyzer.get_issues()
        if issues:
            logger.warning(
                f"N+1 detected in {service_method.__name__}: "
                f"{issues[0].suggestion}"
            )

        return result

    return wrapper
```

### Task 2: Update Services with Eager Loading

**File:** `bot/services/subscription.py`

Add eager loading to prevent N+1 queries:

```python
# Add to imports
from sqlalchemy.orm import selectinload

# Update methods that access relationships
async def get_vip_subscriber_with_relations(self, user_id: int) -> Optional[VIPSubscriber]:
    """
    Obtiene subscriber con eager loading de relaciones.

    Previene N+1 cuando se accede a datos relacionados.
    """
    from bot.database.models import VIPSubscriber

    result = await self.session.execute(
        select(VIPSubscriber)
        .where(VIPSubscriber.user_id == user_id)
        .options(
            selectinload(VIPSubscriber.user),  # Eager load user
            selectinload(VIPSubscriber.tokens)  # Eager load tokens
        )
    )
    return result.scalar_one_or_none()

async def get_all_vip_subscribers_with_users(
    self,
    status: Optional[str] = None,
    limit: int = 100,
    offset: int = 0
) -> List[VIPSubscriber]:
    """
    Obtiene subscribers con eager loading de usuarios.

    Optimizado para listados donde se necesita info de usuario.
    """
    from bot.database.models import VIPSubscriber

    query = select(VIPSubscriber).options(
        selectinload(VIPSubscriber.user)
    )

    if status:
        query = query.where(VIPSubscriber.status == status)

    query = query.limit(limit).offset(offset)

    result = await self.session.execute(query)
    return list(result.scalars().all())
```

**File:** `bot/services/channel.py`

Add similar eager loading patterns for channel-related queries.

### Task 3: Create Migration Script

**File:** `scripts/migrate_to_postgres.py`

Create a comprehensive migration script:

```python
#!/usr/bin/env python3
"""
SQLite to PostgreSQL Migration Script

Migra todos los datos de SQLite a PostgreSQL sin perdida.
Valida integridad despues de la migracion.

Uso:
    python scripts/migrate_to_postgres.py \
        --source bot.db \
        --target postgresql://user:pass@localhost/botdb

    # Dry run (no escribe datos)
    python scripts/migrate_to_postgres.py \
        --source bot.db \
        --target postgresql://... \
        --dry-run

    # Solo validar (sin migrar)
    python scripts/migrate_to_postgres.py \
        --source bot.db \
        --target postgresql://... \
        --validate-only
"""

import argparse
import asyncio
import hashlib
import json
import logging
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Type

from sqlalchemy import create_engine, inspect, select, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from bot.database.base import Base
from bot.database.models import (
    BotConfig, VIPSubscriber, InvitationToken,
    FreeChannelRequest, User, ContentPackage,
    UserInterest, UserRoleChangeLog, PricingTier
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class MigrationResult:
    """Resultado de migracion de una tabla."""
    table: str
    source_count: int
    target_count: int
    checksum_match: bool
    errors: List[str]


class DatabaseMigrator:
    """Migra datos de SQLite a PostgreSQL."""

    # Orden de migracion (respetar dependencias FK)
    MIGRATION_ORDER = [
        BotConfig,
        User,
        VIPSubscriber,
        InvitationToken,
        FreeChannelRequest,
        ContentPackage,
        UserInterest,
        UserRoleChangeLog,
        PricingTier,
    ]

    def __init__(self, source_url: str, target_url: str):
        self.source_url = source_url
        self.target_url = target_url
        self.source_engine = None
        self.target_engine = None
        self.results: List[MigrationResult] = []

    async def initialize(self):
        """Inicializa conexiones a ambas bases de datos."""
        # SQLite source (sync)
        self.source_engine = create_engine(
            self.source_url.replace("sqlite+aiosqlite://", "sqlite://")
                           .replace("sqlite://", "sqlite:///")
        )

        # PostgreSQL target (async)
        self.target_engine = create_async_engine(self.target_url)

        logger.info("Conexiones inicializadas")

    async def create_tables(self):
        """Crea tablas en PostgreSQL."""
        async with self.target_engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        logger.info("Tablas creadas en PostgreSQL")

    async def migrate_table(self, model_class: Type, dry_run: bool = False) -> MigrationResult:
        """
        Migra una tabla especifica.

        Args:
            model_class: Clase del modelo SQLAlchemy
            dry_run: Si True, no escribe datos

        Returns:
            MigrationResult con estadisticas
        """
        table_name = model_class.__tablename__
        logger.info(f"Migrando tabla: {table_name}")

        result = MigrationResult(
            table=table_name,
            source_count=0,
            target_count=0,
            checksum_match=False,
            errors=[]
        )

        try:
            # Leer datos de SQLite
            with self.source_engine.connect() as source_conn:
                rows = source_conn.execute(select(model_class)).all()
                result.source_count = len(rows)

            if dry_run:
                logger.info(f"  [DRY RUN] Leidos {len(rows)} registros")
                return result

            # Escribir a PostgreSQL
            async with AsyncSession(self.target_engine) as session:
                for row in rows:
                    # Convertir Row a dict
                    row_dict = {}
                    for col in model_class.__table__.columns:
                        value = getattr(row, col.name)
                        # Manejar tipos especiales (JSON, etc.)
                        if isinstance(value, (dict, list)):
                            value = json.dumps(value)
                        row_dict[col.name] = value

                    # Crear instancia y merge
                    instance = model_class(**row_dict)
                    await session.merge(instance)

                await session.commit()

            # Verificar conteo
            async with AsyncSession(self.target_engine) as session:
                count_result = await session.execute(
                    select(model_class)
                )
                result.target_count = len(count_result.scalars().all())

            result.checksum_match = result.source_count == result.target_count

            logger.info(f"  Migrados {result.source_count} registros")

        except Exception as e:
            logger.error(f"  Error: {e}")
            result.errors.append(str(e))

        return result

    def calculate_checksum(self, table_name: str) -> str:
        """Calcula checksum de una tabla para validacion."""
        with self.source_engine.connect() as conn:
            result = conn.execute(text(f"SELECT * FROM {table_name} ORDER BY id"))
            data = json.dumps([dict(row._mapping) for row in result], sort_keys=True)
            return hashlib.sha256(data.encode()).hexdigest()

    async def validate_migration(self) -> bool:
        """
        Valida que la migracion fue exitosa.

        Returns:
            True si todas las tablas tienen conteos coincidentes
        """
        logger.info("\nValidando migracion...")

        all_valid = True
        for result in self.results:
            if result.source_count != result.target_count:
                logger.error(
                    f"  {result.table}: "
                    f"{result.source_count} != {result.target_count}"
                )
                all_valid = False
            elif result.errors:
                logger.error(f"  {result.table}: {len(result.errors)} errores")
                all_valid = False
            else:
                logger.info(f"  {result.table}: OK ({result.target_count} registros)")

        return all_valid

    async def migrate(self, dry_run: bool = False) -> bool:
        """
        Ejecuta la migracion completa.

        Returns:
            True si la migracion fue exitosa
        """
        logger.info("Iniciando migracion...")
        logger.info(f"Origen: {self.source_url}")
        logger.info(f"Destino: {self.target_url}")

        if not dry_run:
            await self.create_tables()

        for model_class in self.MIGRATION_ORDER:
            result = await self.migrate_table(model_class, dry_run)
            self.results.append(result)

        if dry_run:
            logger.info("\n[DRY RUN] Validacion de conteos:")
            for result in self.results:
                status = "OK" if result.source_count > 0 else "VACIA"
                logger.info(f"  {result.table}: {result.source_count} registros ({status})")
            return True

        return await self.validate_migration()

    async def generate_report(self) -> Dict:
        """Genera reporte de migracion."""
        return {
            "source": self.source_url,
            "target": self.target_url,
            "tables": [
                {
                    "table": r.table,
                    "source_count": r.source_count,
                    "target_count": r.target_count,
                    "match": r.checksum_match,
                    "errors": r.errors
                }
                for r in self.results
            ],
            "total_source": sum(r.source_count for r in self.results),
            "total_target": sum(r.target_count for r in self.results),
            "success": all(r.checksum_match for r in self.results)
        }

    async def close(self):
        """Cierra conexiones."""
        if self.source_engine:
            self.source_engine.dispose()
        if self.target_engine:
            await self.target_engine.dispose()


async def main():
    parser = argparse.ArgumentParser(
        description="Migra datos de SQLite a PostgreSQL"
    )
    parser.add_argument(
        "--source",
        required=True,
        help="URL de SQLite (ej: sqlite:///bot.db)"
    )
    parser.add_argument(
        "--target",
        required=True,
        help="URL de PostgreSQL (ej: postgresql+asyncpg://user:pass@host/db)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simular sin escribir datos"
    )
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="Solo validar migracion existente"
    )
    parser.add_argument(
        "--output",
        help="Archivo JSON para reporte"
    )

    args = parser.parse_args()

    migrator = DatabaseMigrator(args.source, args.target)

    try:
        await migrator.initialize()

        if args.validate_only:
            # Solo validar
            for model_class in migrator.MIGRATION_ORDER:
                result = await migrator.migrate_table(model_class, dry_run=True)
                migrator.results.append(result)
            success = await migrator.validate_migration()
        else:
            success = await migrator.migrate(dry_run=args.dry_run)

        # Generate report
        report = await migrator.generate_report()

        if args.output:
            Path(args.output).write_text(json.dumps(report, indent=2))
            logger.info(f"\nReporte guardado: {args.output}")

        if success:
            logger.info("\n‚úÖ Migracion completada exitosamente")
            return 0
        else:
            logger.error("\n‚ùå Migracion fallo validacion")
            return 1

    except Exception as e:
        logger.exception("Error en migracion")
        return 1

    finally:
        await migrator.close()


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
```

### Task 4: Add Query Logging Configuration

**File:** `bot/database/engine.py`

Add query logging configuration for development:

```python
# Add to engine creation
import logging

# Enable query logging in debug mode
def create_async_engine_with_logging(database_url: str, debug: bool = False):
    """Crea engine con logging de queries opcional."""
    engine = create_async_engine(
        database_url,
        echo=debug,  # Log all queries if debug=True
        # ... other options
    )

    if debug:
        logging.getLogger("sqlalchemy.engine").setLevel(logging.INFO)

    return engine
```

### Task 5: Create Admin Query Analysis Command

**File:** `bot/handlers/admin/profile.py`

Add query analysis command:

```python
@profile_router.message(Command("analyze_queries"))
async def cmd_analyze_queries(message: Message, session: AsyncSession):
    """
    Analiza queries ejecutadas en una operacion de prueba.

    Detecta N+1 queries y sugiere optimizaciones.
    """
    from bot.utils.query_analyzer import QueryAnalyzer

    await message.answer(
        "üîç <b>Analizando queries...</b>\n"
        "Ejecutando operaciones de prueba y monitoreando queries."
    )

    analyzer = QueryAnalyzer(n_plus_one_threshold=3)
    analyzer.attach_to_session(session)

    try:
        # Run test operations
        container = ServiceContainer(session, message.bot)

        # Test 1: Get VIP subscribers
        await container.subscription.get_all_vip_subscribers(limit=10)

        # Test 2: Get users with roles
        await container.user_management.get_recent_users(limit=10)

        # Analyze
        issues = analyzer.get_issues()
        summary = analyzer.get_summary()

        # Report
        lines = [
            "üìä <b>Resultados del Analisis</b>",
            "",
            f"Total queries: {summary['total_queries']}",
            f"Duracion total: {summary['total_duration_ms']:.1f}ms",
            f"Queries lentas (>100ms): {summary['slow_queries']}",
            f"Problemas N+1 detectados: {summary['n_plus_one_issues']}",
        ]

        if issues:
            lines.extend(["", "‚ùå <b>Problemas detectados:</b>"])
            for issue in issues:
                lines.append(f"\n‚Ä¢ Tabla: <code>{issue.table}</code>")
                lines.append(f"  Queries: {issue.count}")
                lines.append(f"  Sugerencia: {issue.suggestion}")

        await message.answer("\n".join(lines), parse_mode="HTML")

    finally:
        analyzer.detach()
```

## Verification

**Manual Verification:**
1. Run migration dry-run: `python scripts/migrate_to_postgres.py --source bot.db --target postgresql://... --dry-run`
2. Verify row counts match in dry-run output
3. Run actual migration and verify success
4. Run with `--validate-only` and verify checksums match
5. Enable `DEBUG_QUERIES=1` and run `/analyze_queries` in Telegram
6. Verify N+1 warnings appear for unoptimized queries

**Automated Verification:**
```python
async def test_migration():
    """Test migration script works correctly."""
    migrator = DatabaseMigrator(
        "sqlite:///test.db",
        "postgresql+asyncpg://test:test@localhost/test"
    )
    await migrator.initialize()

    success = await migrator.migrate(dry_run=True)
    assert success

async def test_query_analyzer():
    """Test N+1 detection works."""
    from bot.utils.query_analyzer import QueryAnalyzer

    analyzer = QueryAnalyzer(n_plus_one_threshold=2)
    # Simulate queries
    analyzer.query_counts["select_users"] = 5

    issues = analyzer.get_issues()
    assert len(issues) > 0
```

## Anti-Patterns to Avoid

1. **Don't migrate without validation** - Always verify row counts
2. **Don't use lazy loading in loops** - Use selectinload instead
3. **Don't ignore slow query logs** - Investigate queries >100ms
4. **Don't migrate to production directly** - Test on staging first
5. **Don't keep query analysis on in production** - It adds overhead

## Notes

- Migration uses `merge()` to handle conflicts gracefully
- Checksums use SHA256 for data integrity verification
- Query analysis should only run in development/staging
- PostgreSQL URL format: `postgresql+asyncpg://user:pass@host/db`

## References

- SQLAlchemy selectinload: https://docs.sqlalchemy.org/en/20/orm/queryguide/relationships.html
- PostgreSQL migration guide: https://www.postgresql.org/docs/current/migration.html
- asyncpg documentation: https://magicstack.github.io/asyncpg/current/
